import pandas as pd
import matplotlib.pyplot as plt
from urllib import request
import matplotlib
import numpy as np

# Returns the size of the image linked to by img_link
def get_img_size(img_link):
    try: 
        file = request.urlopen(img_link)
    except: 
        print("Bad Link: " + img_link)
        return -1

    size = file.headers.get("content-length")
    
    if size: 
        size = int(size) / 1000 # Convert from bytes to KB
    else: 
        size = 0
    print(img_link + ", " + str(size))
    file.close()
    return(size)

# Parses the data from one link in the visited.json file and updates dataset
def parse_link_data(link_data, dataset, img_sizes): 
    link_count = 0
    image_count = 0
    counting_links = False
    counting_images = False
    
    for line in link_data: 
        if counting_links: 
            if line.strip() == '],':    # End of images
                dataset['num_links'].append(link_count)
                counting_links = False
            else:   # Increment link count
                link_count += 1
        elif counting_images:
            if line.strip() == ']':     # End of links
                dataset['num_images'].append(image_count)
                counting_images = False
            else:   # Get image size and increment image count
                start = line.index('"') + 1
                end = line.rindex('"')
                img_sizes.append(get_img_size(line[start:end]))
                image_count += 1
        else: 
            if line.startswith('  "'):      # Start of data for a page
                end = line.rindex('"')
                dataset["link"].append(line[3:end])
            elif line.startswith('    "size":'):    # Page size
                end = line.rindex(',')
                dataset["size"].append((int(line[12:end])/1000)) # Save size in KB
            elif line.strip() == '"links": [':      # Start of list of links on page
                counting_links = True
            elif line.strip() == '"links": [],':    # Empty list of links
                dataset['num_links'].append(0)
            elif line.strip() == '"images": [':     # Start of list of links on page
                counting_images = True
            elif line.strip() == '"images": []':    # Empty list of images
                dataset['num_images'].append(0)

# Parses the visited.json file generated by the web scraper program and populates dataset
def parse_lines(lines):
    dataset = {
        'link': [],
        'size': [],
        'num_links': [],
        'num_images': []
    }
    img_sizes = []
    link_data = []

    for line in lines: 
        if line.strip() == '{' or line.strip() == '}': # Ignore these lines
            continue
        elif line.strip() == '},': # End of data for that link
            parse_link_data(link_data, dataset, img_sizes)
            link_data = []
        else:
            link_data.append(line)
    return [dataset, img_sizes]

# Generate a .csv named filename from the data in df[column] containing the number of entries that
# fall into each bucket. The buckets values range from low to high. 
def get_weights_csv(df, filename, column, num_buckets, low, high):
    # Populate weights
    weights = [0] * num_buckets
    bucket_size = (high-low)/num_buckets
    for index, row in df.iterrows():
        if row[column] > high or row[column] < low:
            continue
        weights[int((row[column]-low) // bucket_size)] += 1

    # Populate buckets
    buckets = []
    temp = low
    for i in range(num_buckets):
        buckets.append(temp)
        temp += bucket_size

    # Create dataframe 
    data = {
        "buckets": buckets,
        "weights": weights
    }
    result = pd.DataFrame(data)
    path = r'/home/robinpreble/elvis/srg-elvis/tools/scraper/data_analysis/' + filename + '.csv'
    result.to_csv(path, index=False, header=True)

# Generate plots of each attribute in df and img_df
def generate_plots(df, img_df):
    plt.hist(df['size'], bins = 50, range = [0, 2500], color='blue', edgecolor='black')
    plt.xlabel('Page Size (KB)')
    plt.ylabel('No. of Pages')
    plt.title('Page Size')
    plt.savefig('page_size.pdf')
    plt.close()

    plt.hist(df['num_links'], bins = 50, range = [0, 1500], color='blue', edgecolor='black')
    plt.xlabel('No. of Links on Page')
    plt.ylabel('No. of Pages')
    plt.title('No. of Links')
    plt.savefig('num_links.pdf')
    plt.close()

    plt.hist(df['num_images'], bins = 50, range = [0, 400], color='blue', edgecolor='black')
    plt.xlabel('No. of Images on Page')
    plt.ylabel('No. of Pages')
    plt.title('No. of Images')
    plt.savefig('num_images.pdf')
    plt.close()

    plt.hist(img_df, bins = 50, range = [-1, 350], color='blue', edgecolor='black')
    plt.xlabel('Image Size (KB)')
    plt.ylabel('No. of Images')
    plt.title('Image Sizes')
    plt.savefig('image_sizes.pdf')
    plt.close()

def main():
    # Read and parse file
    # f = open("visited.json", "r")
    # lines = f.readlines()
    # f.close()
    # data = parse_lines(lines)

    # Convert to dataframes and save as .csv files
    # dataset = data[0]
    # img_sizes = data[1]
    # df = pd.DataFrame(dataset)
    # img_df = pd.DataFrame(img_sizes)
    # df.to_csv(r'/home/prebler/elvis/scraper-min/data_analysis/dataframe.csv', index=False, header=True)
    # img_df.to_csv(r'/home/prebler/elvis/scraper-min/data_analysis/img_dataframe.csv', index=False, header=True)
    
    # Open dataframe files
    df = pd.read_csv('dataframe.csv')
    img_df = pd.read_csv('img_dataframe.csv')

    # Generate weights .csv's
    get_weights_csv(df, 'page_size', 1, 50, 0, 2500)
    get_weights_csv(df, 'num_links', 2, 50, 0, 1500)
    get_weights_csv(df, 'num_images', 3, 50, 0, 400)
    get_weights_csv(img_df, "image_size", 0, 50, 1, 350)

if __name__ == "__main__":
    main()